{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bIrh7srKtXKm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import json\n",
        "import logging\n",
        "import pickle\n",
        "from typing import Any, Mapping, Iterable, Union, List, Callable, Optional\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "\n",
        "def resolve_globs(glob_paths: Union[str, Iterable[str]]):\n",
        "    \"\"\"Returns filepaths corresponding to input filepath pattern(s).\"\"\"\n",
        "    filepaths = []\n",
        "    if isinstance(glob_paths, str):\n",
        "        glob_paths = [glob_paths]\n",
        "\n",
        "    for path in glob_paths:\n",
        "        filepaths.extend(glob.glob(path))\n",
        "\n",
        "    return filepaths\n",
        "\n",
        "\n",
        "def read_jsonlines(filename: str) -> Iterable[Mapping[str, Any]]:\n",
        "    \"\"\"Yields an iterable of Python dicts after reading jsonlines from the input file.\"\"\"\n",
        "    file_size = os.path.getsize(filename)\n",
        "    with open(filename) as fp:\n",
        "        for line in tqdm(fp.readlines(), desc=f'Reading JSON lines from {filename}', unit='lines'):\n",
        "            try:\n",
        "                example = json.loads(line)\n",
        "                yield example\n",
        "            except json.JSONDecodeError as ex:\n",
        "                logging.error(f'Input text: \"{line}\"')\n",
        "                logging.error(ex.args)\n",
        "                raise ex\n",
        "\n",
        "\n",
        "def load_jsonlines(filename: str) -> List[Mapping[str, Any]]:\n",
        "    \"\"\"Returns a list of Python dicts after reading jsonlines from the input file.\"\"\"\n",
        "    return list(read_jsonlines(filename))\n",
        "\n",
        "\n",
        "def write_jsonlines(objs: Iterable[Mapping[str, Any]], filename: str, to_dict: Callable = lambda x: x):\n",
        "    \"\"\"Writes a list of Python Mappings as jsonlines at the input file.\"\"\"\n",
        "    with open(filename, 'w') as fp:\n",
        "        for obj in tqdm(objs, desc=f'Writing JSON lines at {filename}'):\n",
        "            fp.write(json.dumps(to_dict(obj)))\n",
        "            fp.write('\\n')\n",
        "\n",
        "\n",
        "def read_json(filename: str) -> Mapping[str, Any]:\n",
        "    \"\"\"Returns a Python dict representation of JSON object at input file.\"\"\"\n",
        "    with open(filename) as fp:\n",
        "        return json.load(fp)\n",
        "\n",
        "\n",
        "def write_json(obj: Mapping[str, Any], filename: str):\n",
        "    \"\"\"Writes a Python Mapping at the input file in JSON format.\"\"\"\n",
        "    with open(filename, 'w') as fp:\n",
        "        json.dump(obj, fp)\n",
        "\n",
        "\n",
        "def print_json(d, indent=4):\n",
        "    print(json.dumps(d, indent=indent))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVye2WNetc45"
      },
      "outputs": [],
      "source": [
        "# coding=utf-8\n",
        "# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n",
        "# Adapted by the CMSC828A-Spring2023-Development team\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "\"\"\" Finetuning an ensemble of ðŸ¤— Transformers models for image classification.\"\"\"\n",
        "import argparse\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import torch.nn as nn\n",
        "\n",
        "import datasets\n",
        "from datasets import DatasetDict\n",
        "import evaluate\n",
        "import torch\n",
        "from accelerate import Accelerator\n",
        "from accelerate.logging import get_logger\n",
        "from accelerate.utils import set_seed\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoConfig, AutoImageProcessor, AutoModelForImageClassification, SchedulerType, get_scheduler\n",
        "from transformers.utils.versions import require_version\n",
        "\n",
        "\n",
        "logger = get_logger(__name__)\n",
        "\n",
        "require_version(\"datasets>=2.0.0\", \"To fix: pip install -r examples/pytorch/image-classification/requirements.txt\")\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(description=\"Train an ensemble of image classfication models on the Maysee/tiny-imagenet dataset\")\n",
        "    parser.add_argument(\n",
        "        \"--eval_only\",\n",
        "        default = False,\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to run evaluation only. Assumes previously saved models are in the output directory.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--model_name_or_path\",\n",
        "        type=str,\n",
        "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
        "        default=\"microsoft/resnet-18\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--per_device_train_batch_size\",\n",
        "        type=int,\n",
        "        default=32,\n",
        "        help=\"Batch size (per device) for the training dataloader.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--per_device_eval_batch_size\",\n",
        "        type=int,\n",
        "        default=32,\n",
        "        help=\"Batch size (per device) for the evaluation dataloader.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--learning_rate\",\n",
        "        type=float,\n",
        "        default=1e-4,\n",
        "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
        "    )\n",
        "    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"Weight decay to use.\")\n",
        "    parser.add_argument(\"--num_train_epochs\", type=int, default=50, help=\"Total number of training epochs to perform.\")\n",
        "    parser.add_argument(\n",
        "        \"--lr_scheduler_type\",\n",
        "        type=SchedulerType,\n",
        "        default=\"linear\",\n",
        "        help=\"The scheduler type to use.\",\n",
        "        choices=[\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"],\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--num_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
        "    )\n",
        "    parser.add_argument(\"--max_train_steps_per_epoch\", type=int, default=None, help=\"Total number of training steps to perform per epoch, per task, rather than covering the entire task dataset.\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default='./output', required=False, help=\"Where to store the final model.\")\n",
        "    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")    \n",
        "    parser.add_argument(\n",
        "        \"--with_tracking\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether to enable experiment trackers for logging.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--report_to\",\n",
        "        type=str,\n",
        "        default=\"wandb\",\n",
        "        help=(\n",
        "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`,'\n",
        "            ' `\"wandb\"`, `\"comet_ml\"` and `\"clearml\"`. Use `\"all\"` (default) to report to all integrations.'\n",
        "            \"Only applicable when `--with_tracking` is passed.\"\n",
        "        ),\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--ignore_mismatched_sizes\",\n",
        "        action=\"store_true\",\n",
        "        help=\"Whether or not to enable to load a pretrained model whose head dimensions are different.\",\n",
        "    )\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    # Sanity checks\n",
        "    if args.output_dir is not None:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "\n",
        "    return args\n",
        "\n",
        "# Ensemble aggegation method that takes in a test example and either selects\n",
        "# a single model using a score based scheme such as \"confidence\" or \"entropy\"\n",
        "# or aggregates the predictions of all models using an real aggregation function\n",
        "# or neighbor scheme. If the neighbor method is used, then a \n",
        "# kNN model fit to the featurized training data is also required.\n",
        "def ensemble_prediction(test_example, \n",
        "                        task_models, \n",
        "                        aggregation_method=\"confidence\",\n",
        "                        knn_model=None):\n",
        "    if aggregation_method == \"confidence\":\n",
        "        # for each model, get the confidence score for the top prediction\n",
        "        # then select the prediction from the model with the highest confidence score\n",
        "        # as the final prediction\n",
        "\n",
        "        raise NotImplementedError(\"confidence based selection not yet implemented\")\n",
        "\n",
        "    elif aggregation_method == \"entropy\":\n",
        "        # for each model, get the entropy of the prediction distribution\n",
        "        # then select the prediction from the model with the lowest output entropy\n",
        "        # as the final prediction\n",
        "\n",
        "        raise NotImplementedError(\"entropy based selection not yet implemented\")\n",
        "\n",
        "    elif aggregation_method == \"neighbor_majority\":\n",
        "        # MEEE\n",
        "        raise NotImplementedError(\"knn based majority voting aggregation not yet implemented\")\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(\"Invalid aggregation method\")\n",
        "\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    # Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
        "    # If we're using tracking, we also need to initialize it here and it will by default pick up all supported trackers\n",
        "    # in the environment\n",
        "    accelerator_log_kwargs = {}\n",
        "\n",
        "    if args.with_tracking:\n",
        "        accelerator_log_kwargs[\"log_with\"] = args.report_to\n",
        "        accelerator_log_kwargs[\"logging_dir\"] = args.output_dir\n",
        "\n",
        "    accelerator = Accelerator(**accelerator_log_kwargs)\n",
        "\n",
        "    logger.info(accelerator.state)\n",
        "    # Make one log on every process with the configuration for debugging.\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        "    )\n",
        "    logger.info(accelerator.state, main_process_only=False)\n",
        "    if accelerator.is_local_main_process:\n",
        "        datasets.utils.logging.set_verbosity_warning()\n",
        "        transformers.utils.logging.set_verbosity_info()\n",
        "    else:\n",
        "        datasets.utils.logging.set_verbosity_error()\n",
        "        transformers.utils.logging.set_verbosity_error()\n",
        "\n",
        "    # If passed along, set the training seed now.\n",
        "    if args.seed is not None:\n",
        "        set_seed(args.seed)\n",
        "\n",
        "    # Get the datasets\n",
        "\n",
        "    # Task split creation\n",
        "    task_split_meta = read_json(\"./task_splits.json\")\n",
        "\n",
        "    task_to_class_list = task_split_meta[\"task_to_class_list\"]\n",
        "    task_to_class_list = {k: set(v) for k, v in task_to_class_list.items()} # just to make membership check faster\n",
        "\n",
        "    # make label_to_task dict using the task_to_class_list for oracle method\n",
        "    label_to_task = {}\n",
        "    for task in task_to_class_list:\n",
        "        for label in task_to_class_list[task]:\n",
        "            label_to_task[label] = task\n",
        "\n",
        "    # Prepare label mappings.\n",
        "    # these are originally from the tinyimagenet dataset\n",
        "    label2id = task_split_meta[\"label2id\"]\n",
        "    id2label = task_split_meta[\"id2label\"]\n",
        "\n",
        "    # if args.preprocessed_task_train_datasets_path is None:\n",
        "    # Downloading and loading a dataset from the hub.\n",
        "    dataset = load_dataset(\"Maysee/tiny-imagenet\", task=\"image-classification\")\n",
        "\n",
        "    task_train_datasets = DatasetDict(\n",
        "        {k: dataset[\"train\"].filter(lambda x: x[\"labels\"] in task_to_class_list[k]) for k in task_to_class_list.keys()}\n",
        "    )\n",
        "    \n",
        "    task_eval_datasets = DatasetDict(\n",
        "        {k: dataset[\"valid\"].filter(lambda x: x[\"labels\"] in task_to_class_list[k]) for k in task_to_class_list.keys()}\n",
        "    )\n",
        "    combined_eval_dataset = dataset[\"valid\"]\n",
        "    \n",
        "    \n",
        "    task_train_ensemble_datasets = {}\n",
        "    task_train_knn_datasets = {}\n",
        "    import json\n",
        "    for k in task_train_datasets.keys():\n",
        "        task_train_ensemble_datasets[k], task_train_knn_datasets[k] = task_train_datasets[k].train_test_split(test_size=0.99).values()\n",
        "    torch.save(task_train_ensemble_datasets, \"task_train_ensemble_datasets.pt\")\n",
        "    torch.save(task_train_knn_datasets, \"task_train_knn_datasets.pt\")\n",
        "    \n",
        "    # Load pretrained model and image processor\n",
        "    #\n",
        "    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n",
        "    # download model & vocab.\n",
        "    config = AutoConfig.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        num_labels=len(label2id),\n",
        "        i2label=id2label,\n",
        "        label2id=label2id,\n",
        "        finetuning_task=\"image-classification\",\n",
        "    )\n",
        "    image_processor = AutoImageProcessor.from_pretrained(args.model_name_or_path)\n",
        "    task_models = {k: AutoModelForImageClassification.from_config(config=config) for k in task_to_class_list.keys()}\n",
        "\n",
        "    # Preprocessing the datasets\n",
        "    # Define torchvision transforms to be applied to each image.\n",
        "    if \"shortest_edge\" in image_processor.size:\n",
        "        size = image_processor.size[\"shortest_edge\"]\n",
        "    else:\n",
        "        size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        "    # This is the same normalization as standard imagenet data and so we should use it for tinyimagenet as well\n",
        "    normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "    train_transforms = Compose(\n",
        "        [\n",
        "            RandomResizedCrop(size),\n",
        "            RandomHorizontalFlip(),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "    val_transforms = Compose(\n",
        "        [\n",
        "            Resize(size),\n",
        "            CenterCrop(size),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def preprocess_train(example_batch):\n",
        "        \"\"\"Apply _train_transforms across a batch.\"\"\"\n",
        "        example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
        "        return example_batch\n",
        "\n",
        "    def preprocess_val(example_batch):\n",
        "        \"\"\"Apply _val_transforms across a batch.\"\"\"\n",
        "        example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
        "        return example_batch\n",
        "\n",
        "    with accelerator.main_process_first():\n",
        "        task_train_ensemble_datasets = DatasetDict(\n",
        "            {k: v.with_transform(preprocess_train) for k, v in task_train_ensemble_datasets.items()}\n",
        "        )\n",
        "        task_train_knn_datasets = DatasetDict(\n",
        "            {k: v.with_transform(preprocess_train) for k, v in task_train_knn_datasets.items()}\n",
        "        )\n",
        "        task_eval_datasets = DatasetDict(\n",
        "            {k: v.with_transform(preprocess_val) for k, v in task_eval_datasets.items()}\n",
        "        )\n",
        "        combined_eval_dataset = combined_eval_dataset.with_transform(preprocess_val)\n",
        "\n",
        "    # DataLoaders creation:\n",
        "    def collate_fn(examples):\n",
        "        pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "        labels = torch.tensor([example[\"labels\"] for example in examples])\n",
        "        return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "    task_train_ensemble_dataloaders = {\n",
        "        task: DataLoader(\n",
        "            task_train_ensemble_datasets[task], shuffle=True, collate_fn=collate_fn, batch_size=args.per_device_train_batch_size\n",
        "        )\n",
        "        for task in task_train_ensemble_datasets\n",
        "    }\n",
        "    \n",
        "    task_train_knn_dataloaders = {\n",
        "        task: DataLoader(\n",
        "            task_train_knn_datasets[task], shuffle=True, collate_fn=collate_fn, batch_size=args.per_device_train_batch_size\n",
        "        )\n",
        "        for task in task_train_knn_datasets\n",
        "    }\n",
        "    \n",
        "    \n",
        "    task_eval_dataloaders = {\n",
        "        task: DataLoader(\n",
        "            task_eval_datasets[task], collate_fn=collate_fn, batch_size=args.per_device_eval_batch_size\n",
        "        )\n",
        "        for task in task_eval_datasets\n",
        "    }\n",
        "    \n",
        "    combined_eval_dataloader = DataLoader(\n",
        "        combined_eval_dataset, collate_fn=collate_fn, batch_size=args.per_device_eval_batch_size\n",
        "    )\n",
        "\n",
        "    # make one optimizer per task model\n",
        "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "    task_optimizers = {\n",
        "        task: torch.optim.AdamW(\n",
        "            [\n",
        "                {\n",
        "                    \"params\": [p for n, p in task_models[task].named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": args.weight_decay,\n",
        "                },\n",
        "                {\n",
        "                    \"params\": [p for n, p in task_models[task].named_parameters() if any(nd in n for nd in no_decay)],\n",
        "                    \"weight_decay\": 0.0,\n",
        "                },\n",
        "            ],\n",
        "            lr=args.learning_rate,\n",
        "        )\n",
        "        for task in task_models\n",
        "    }\n",
        "\n",
        "    if args.max_train_steps_per_epoch is None:\n",
        "        task_steps_per_epoch = {\n",
        "            task: len(task_train_ensemble_dataloaders[task])\n",
        "            for task in task_train_ensemble_dataloaders\n",
        "        }\n",
        "    else:\n",
        "        task_steps_per_epoch = {\n",
        "            task: min(args.max_train_steps_per_epoch, len(task_train_ensemble_dataloaders[task]))\n",
        "            for task in task_train_ensemble_dataloaders\n",
        "        }\n",
        "\n",
        "    # make one lr_scheduler per task as the num steps will be different\n",
        "    lr_schedulers = {\n",
        "        task: get_scheduler(\n",
        "            name=args.lr_scheduler_type,\n",
        "            optimizer=task_optimizers[task],\n",
        "            num_warmup_steps=args.num_warmup_steps,\n",
        "            num_training_steps=task_steps_per_epoch[task] * args.num_train_epochs,\n",
        "        )\n",
        "        for task in task_train_ensemble_datasets\n",
        "    }\n",
        "\n",
        "    # prepare the train and eval dataloaders and lr_schedulers, optimizers and models\n",
        "    task_train_ensemble_dataloaders = {task: accelerator.prepare(task_train_ensemble_dataloaders[task]) for task in task_train_ensemble_dataloaders}\n",
        "    task_train_knn_dataloaders = {task: accelerator.prepare(task_train_knn_dataloaders[task]) for task in task_train_knn_dataloaders}\n",
        "    task_eval_dataloaders = {task: accelerator.prepare(task_eval_dataloaders[task]) for task in task_eval_dataloaders}\n",
        "    combined_eval_dataloader = accelerator.prepare(combined_eval_dataloader)\n",
        "    \n",
        "    lr_schedulers = {task: accelerator.prepare(lr_schedulers[task]) for task in lr_schedulers}\n",
        "    task_optimizers = {task: accelerator.prepare(task_optimizers[task]) for task in task_optimizers}\n",
        "    # task_models = {task: accelerator.prepare(task_models[task]) for task in task_models}\n",
        "\n",
        "    # send all models to cpu, we will move them to the correct device when we need them\n",
        "    task_models = {task: task_models[task].cpu() for task in task_models}\n",
        "\n",
        "    total_train_steps = sum(task_steps_per_epoch.values()) * args.num_train_epochs\n",
        "\n",
        "    # We need to initialize the trackers we use, and also store our configuration.\n",
        "    # The trackers initializes automatically on the main process.\n",
        "    if args.with_tracking:\n",
        "        experiment_config = vars(args)\n",
        "        # TensorBoard cannot log Enums, need the raw value\n",
        "        experiment_config[\"lr_scheduler_type\"] = experiment_config[\"lr_scheduler_type\"].value\n",
        "        accelerator.init_trackers(\"image_classification_no_trainer\", experiment_config)\n",
        "\n",
        "    # Get the metric function\n",
        "    metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "    ##### TRAINING #####\n",
        "\n",
        "    if not args.eval_only:\n",
        "\n",
        "        # Train!\n",
        "        logger.info(\"***** Running training *****\")\n",
        "        logger.info(f\"  Instantaneous batch size per device = {args.per_device_train_batch_size}\")\n",
        "        logger.info(f\"  Num Epochs per task = {args.num_train_epochs}\")\n",
        "        logger.info(f\"  Num optimization steps per task = {task_steps_per_epoch}\")\n",
        "        logger.info(f\"  Total optimization steps across all tasks = {total_train_steps}\")\n",
        "\n",
        "        for task in task_train_ensemble_dataloaders:\n",
        "            train_dataloader = task_train_ensemble_dataloaders[task]\n",
        "            eval_dataloader = task_eval_dataloaders[task]\n",
        "            lr_scheduler = lr_schedulers[task]\n",
        "            optimizer = task_optimizers[task]\n",
        "            \n",
        "            # pull out model, move to gpu, then return to cpu after finished with all epochs\n",
        "            model = task_models[task]\n",
        "            model.to(accelerator.device)\n",
        "\n",
        "            completed_steps = 0\n",
        "            total_train_step_this_task = task_steps_per_epoch[task] * args.num_train_epochs\n",
        "            task_progress_bar = tqdm(range(total_train_step_this_task), disable=not accelerator.is_local_main_process, desc=f\"Task {task} training\")\n",
        "            for epoch in range(args.num_train_epochs):\n",
        "                model.train()\n",
        "                if args.with_tracking:\n",
        "                    total_loss = 0\n",
        "                for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "                    outputs = model(**batch)\n",
        "                    loss = outputs.loss\n",
        "                    # We keep track of the loss at each epoch\n",
        "                    if args.with_tracking:\n",
        "                        total_loss += loss.detach().float()\n",
        "                    accelerator.backward(loss)\n",
        "                    optimizer.step()\n",
        "                    lr_scheduler.step()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                    task_progress_bar.update(1)\n",
        "                    completed_steps += 1\n",
        "\n",
        "                    if step >= task_steps_per_epoch[task]-1:\n",
        "                        break\n",
        "                \n",
        "                if completed_steps >= total_train_step_this_task:\n",
        "                    task_progress_bar.close()\n",
        "\n",
        "                ####### PER TASK EVALUATION #######\n",
        "                model.eval()\n",
        "                for step, batch in enumerate(eval_dataloader):\n",
        "                    with torch.no_grad():\n",
        "                        outputs = model(**batch)\n",
        "                    predictions = outputs.logits.argmax(dim=-1)\n",
        "                    predictions, references = accelerator.gather_for_metrics((predictions, batch[\"labels\"]))\n",
        "                    metric.add_batch(\n",
        "                        predictions=predictions,\n",
        "                        references=references,\n",
        "                    )\n",
        "\n",
        "                eval_metric = metric.compute()\n",
        "                logger.info(f\"Task {task} epoch {epoch}: {eval_metric}\")\n",
        "\n",
        "                if args.with_tracking:\n",
        "                    accelerator.wait_for_everyone()\n",
        "                    metrics_to_log = {\n",
        "                            f\"task_{task}_val_accuracy\": eval_metric[\"accuracy\"],\n",
        "                            f\"task_{task}_train_loss\": total_loss.item() / task_steps_per_epoch[task],\n",
        "                            f\"task_{task}_lr\": optimizer.param_groups[0][\"lr\"],\n",
        "                            \"epoch\": epoch,\n",
        "                            \"step\": completed_steps,\n",
        "                        }\n",
        "                    accelerator.log(\n",
        "                        metrics_to_log,\n",
        "                    )\n",
        "            model.to(\"cpu\")\n",
        "\n",
        "        if args.output_dir is not None:\n",
        "            accelerator.wait_for_everyone()\n",
        "            # unwrap and save each model\n",
        "            for task in task_models:\n",
        "                unwrapped_model = accelerator.unwrap_model(task_models[task])\n",
        "                unwrapped_model.save_pretrained(\n",
        "                    f\"{args.output_dir}/task_{task}\", is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
        "                )\n",
        "\n",
        "            if accelerator.is_main_process:\n",
        "                image_processor.save_pretrained(args.output_dir)\n",
        "    \n",
        "\n",
        "    # load the previously saved models, will fail if not trained and saved as expected above\n",
        "    class Hook:\n",
        "        def __init__(self):\n",
        "            self.feats = None\n",
        "        \n",
        "        def hook_fn(self, mod, inp, out):\n",
        "            self.feats = inp\n",
        "    for task in task_models:\n",
        "        task_models[task] = accelerator.unwrap_model(task_models[task])\n",
        "        task_models[task].load_state_dict(torch.load(f\"{args.output_dir}/task_{task}/pytorch_model.bin\"))\n",
        "        task_models[task].to(accelerator.device)\n",
        "        task_models[task].eval()\n",
        "\n",
        "    m = 0\n",
        "    all_all_all_outputs = []\n",
        "    for task in task_train_knn_dataloaders:\n",
        "        train_knn_dataloader = task_train_knn_dataloaders[task]\n",
        "        all_all_outputs = []\n",
        "        for step, batch in enumerate(train_knn_dataloader):\n",
        "            all_outputs = []\n",
        "            for the_task in range(1, 6):\n",
        "                model = task_models[str(the_task)]\n",
        "                hook = Hook()\n",
        "                model.classifier.register_forward_hook(hook.hook_fn)\n",
        "                model.to(accelerator.device)\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(**batch)\n",
        "                all_outputs.append(outputs.cpu().detach())\n",
        "                if the_task == 1:\n",
        "                    model_feats = hook.feats[0].squeeze(2).squeeze(2)\n",
        "                else:\n",
        "                    model_feats = torch.cat([model_feats, hook.feats[0].squeeze(2).squeeze(2)], dim=1)\n",
        "            all_all_outputs.append(all_outputs)\n",
        "            if m == 0:\n",
        "                combined_feats = model_feats\n",
        "            else:\n",
        "                combined_feats = torch.cat([combined_feats, model_feats], dim=0)\n",
        "                \n",
        "            if m == 0:\n",
        "                labels = batch['labels']\n",
        "            else:\n",
        "                labels = torch.cat([labels, batch['labels']], dim = 0)\n",
        "            m += 1\n",
        "        all_all_all_outputs.append(all_outputs)\n",
        "\n",
        "    with open('./model_outputs_train_knn.pkl', 'wb+') as fp:\n",
        "        pickle.dump(all_all_all_outputs, fp)\n",
        "    with open('./combined_feats_train_knn.pkl', 'wb+') as fp:\n",
        "        pickle.dump(combined_feats, fp)\n",
        "    with open('./labels_train_knn.pkl', 'wb+') as fp:\n",
        "        pickle.dump(labels, fp)\n",
        "\n",
        "    \n",
        "\n",
        "    # set the seed so that any randomness in the eval steps is reproducible\n",
        "    if args.seed is not None:\n",
        "        set_seed(args.seed)\n",
        "    from torch import nn\n",
        "    class LinearClassifier(nn.Module):\n",
        "        \"\"\"Linear layer to train on top of frozen features\"\"\"\n",
        "        def __init__(self, dim, num_labels=1000):\n",
        "            super(LinearClassifier, self).__init__()\n",
        "            self.num_labels = num_labels\n",
        "            self.linear = nn.Linear(dim, num_labels)\n",
        "            self.linear.weight.data.normal_(mean=0.0, std=0.01)\n",
        "            self.linear.bias.data.zero_()\n",
        "\n",
        "        def forward(self, x):\n",
        "            # flatten\n",
        "            x = x.view(x.size(0), -1)\n",
        "\n",
        "            # linear layer\n",
        "            return self.linear(x)\n",
        "    linear_classifier = LinearClassifier(2560, num_labels=200)\n",
        "    linear_classifier = linear_classifier.cuda()\n",
        "    optimizer = torch.optim.SGD(\n",
        "            linear_classifier.parameters(),\n",
        "            0.15 * (16) / 256., # linear scaling rule\n",
        "            momentum=0.9,\n",
        "            weight_decay=0,)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000, eta_min=0)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for epoch in range(1000):\n",
        "        preds = linear_classifier(combined_feats.cuda(non_blocking=True))\n",
        "        loss = criterion(preds, labels.long().cuda(non_blocking=True))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        _, predicted = torch.max(preds.detach(), 1)\n",
        "        correct = (predicted == labels.long().cuda(non_blocking=True)).sum().item()\n",
        "        print(correct/len(labels))\n",
        "        \n",
        "    ###### COMBINED EVALUATION ######\n",
        "    # evaluate all individual models on combined eval dataset\n",
        "    # pull out model, move to gpu, then return to cpu after finished\n",
        "    \n",
        "    \n",
        "    m = 0\n",
        "    all_all_outputs = []\n",
        "    for step, batch in enumerate(combined_eval_dataloader):\n",
        "        all_outputs = []\n",
        "        for the_task in range(1, 6):\n",
        "            model = task_models[str(the_task)]\n",
        "            hook = Hook()\n",
        "            model.classifier.register_forward_hook(hook.hook_fn)\n",
        "            model.to(accelerator.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**batch)\n",
        "            all_outputs.append(outputs.cpu().detach())\n",
        "            if the_task == 1:\n",
        "                model_feats = hook.feats[0].squeeze(2).squeeze(2)\n",
        "            else:\n",
        "                model_feats = torch.cat([model_feats, hook.feats[0].squeeze(2).squeeze(2)], dim=1)\n",
        "        all_all_outputs.append(all_outputs)\n",
        "        if m == 0:\n",
        "            combined_feats_eval = model_feats\n",
        "        else:\n",
        "            combined_feats_eval = torch.cat([combined_feats, model_feats], dim=0)\n",
        "\n",
        "        if m == 0:\n",
        "            labels_eval = batch['labels']\n",
        "        else:\n",
        "            labels_eval = torch.cat([labels, batch['labels']], dim = 0)\n",
        "        m += 1\n",
        "        all_combined_features.append()\n",
        "    print(combined_feats_eval.shape)\n",
        "    print(labels_eval.shape)\n",
        "    with open('./model_outputs_eval.pkl', 'wb+') as fp:\n",
        "        pickle.dump(all_all_outputs, fp)\n",
        "    with open('./combined_feats_eval.pkl', 'wb+') as fp:\n",
        "        pickle.dump(combined_feats_eval, fp)\n",
        "    with open('./labels_eval.pkl', 'wb+') as fp:\n",
        "        pickle.dump(labels_eval, fp)\n",
        "            \n",
        "    linear_classifier.eval()\n",
        "    preds = linear_classifier(combined_feats_eval.cuda(non_blocking=True))\n",
        "    _, predicted = torch.max(preds.detach(), 1)\n",
        "    correct = (predicted == labels_eval.long().cuda(non_blocking=True)).sum().item()\n",
        "    print(correct/len(labels_eval))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if args.with_tracking:\n",
        "        accelerator.end_training()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
